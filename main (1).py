# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140UIHQqT3e6v1UuSH5CYFJJgGTyM8k0a
"""

!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

data = pd.read_csv('aug_train.csv')
test = pd.read_csv('aug_test.csv')

data.head(5)
# target = 0 means those employees are not looking for a job change.
# target = 1 means those employees are looking for a job change.

data.dtypes

data.isnull().sum()

# Frequency of each category separeted by label.
plt.figure(figsize=[20,25])
features = ['gender', 'relevent_experience', 'enrolled_university',
            'education_level', 'major_discipline', 'experience',
            'company_size', 'company_type', 'last_new_job']

n = 1
for f in features:
    plt.subplot(5,2,n)
    sns.countplot(x=f, hue='target', alpha = 0.7, data=data)
    plt.title('countplot fo {} by target'.format(f))
    n = n+1
plt.tight_layout()
plt.show()

# 1. Data Treatment

# Churn vs Normal
counts = data.target.value_counts()
not_change = counts[0]
change = counts[1]
prec_not_change = (not_change / (not_change + change)) * 100
prec_change = (change / (not_change + change)) * 100
print("Not looking for a job change: {} ({:.2f}%).".format(not_change, prec_not_change))
print("Looking for a job change: {} ({:.2f}%).".format(change, prec_change))

# If we want to select the the numerical columns only or categorical columns only from the larger dataset,
# We can use the following code:
np.array(data.columns[data.dtypes != 'object'])

import copy

df_train = copy.deepcopy(data)
df_test = copy.deepcopy(test)

cols = np.array(data.columns[data.dtypes != 'object'])
for i in df_train.columns:
    if i not in cols:
        df_train[i] = df_train[i].map(str)
        df_test[i] = df_test[i].map(str)
df_train.drop(columns = cols, inplace=True)
df_test.drop(columns = np.delete(cols, len(cols)-1), inplace=True)

df_train.dtypes

from sklearn.preprocessing import LabelEncoder
from collections import defaultdict

# Building the dict funtion
cols = np.array(data.columns[data.dtypes != 'object'])
d = defaultdict(LabelEncoder)

# Only for categorical columns apply dict by calling the fit_transform function.
df_train = df_train.apply(lambda x: d[x.name].fit_transform(x))
df_test = df_test.apply(lambda x: d[x.name].fit_transform(x))
df_train[cols] = data[cols]
df_test[np.delete(cols, len(cols)-1)] = test[np.delete(cols, len(cols)-1)]

df_train.dtypes

import matplotlib.style as style
style.use('ggplot')
sns.set_style('whitegrid')
plt.subplots(figsize=(12,6))

# Generate the upper mask triangle
mask = np.zeros_like(df_train.corr(),dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
sns.heatmap(df_train.corr(), cmap = sns.diverging_palette(220, 20, n=200), annot=True,
                                                          mask = mask, center=0)
plt.title('Heat map of all the features of train dataset', fontsize=20)

# Visualization of the features which has positive and negative correlation.
# FOr that we will use box plot.

f, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (20,15))

f.suptitle('features with high positive and negative correlation', size=30)
sns.boxplot(x='target', y='city', data = df_train, ax= axes[0,0])
sns.boxplot(x='target', y='gender', data = df_train, ax= axes[0,1])
sns.boxplot(x='target', y='relevent_experience', data = df_train, ax= axes[0,2])
sns.boxplot(x='target', y='enrolled_university', data = df_train, ax= axes[1,0])
sns.boxplot(x='target', y='education_level', data = df_train, ax= axes[1,1])
sns.boxplot(x='target', y='company_size', data = df_train, ax= axes[1,2])
sns.boxplot(x='target', y='company_type', data = df_train, ax= axes[2,0])
sns.boxplot(x='target', y='last_new_job', data = df_train, ax= axes[2,1])
sns.boxplot(x='target', y='experience', data = df_train, ax= axes[2,2])



################# or using loop we can do it like this:- #################
# features = [
#     'city', 'gender', 'relevent_experience',
#     'enrolled_university', 'education_level', 'company_size',
#     'company_type', 'last_new_job', 'experience'
# ]

# # subplots creating
# f, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,15))
# f.suptitle('features with high positive and negative correlation', size=30)

# # Flatten axes for easy iteration
# axes = axes.flatten()

# for i, feature in enumerate(features):
#     sns.boxplot(x='target', y=feature, data=df_train, ax=axes[i])

# plt.tight_layout(rect=[0, 0, 1, 0.96])
# plt.show()

# Doing the same things like creating the heat map again for the test datasets and all such processing.

import matplotlib.style as style
style.use('ggplot')
sns.set_style('whitegrid')
plt.subplots(figsize=(12,6))

# Generate the upper mask triangle
mask = np.zeros_like(df_test.corr(),dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

corr = df_test.corr()
sns.heatmap(corr[(corr.abs() > 0.01)],
            cmap = sns.diverging_palette(220, 20, n=200),
            annot=True,
            mask = mask, center=0)
plt.title('Heat map of all the features of test dataset', fontsize=20)

df_train['target'].value_counts()
# we can see that the dataset is imbalanced. Majority of the employees are not looking for a job change.
# So to overcome this, SMOTE(Synthetic Minority Over-sampling Technique) will be use.

# 2. Dealing with imbalanced data using SMOTE technique.

df_train.columns

ftrain = ['city', 'gender', 'relevent_experience', 'enrolled_university',
       'education_level', 'company_size','company_type', 'city_development_index',
       'training_hours', 'target']

ftest = ['city', 'gender', 'relevent_experience', 'enrolled_university',
       'education_level', 'company_size','company_type', 'city_development_index',
       'training_hours']

def Definedata():
    # Defining the data as it is the imbalanced data.
    data2 = df_train[ftrain]
    x = data2.drop(columns = ['target']).values
    y = data2['target'].values
    return x,y

from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
from matplotlib import pyplot as plt
from numpy import where

def SMOTE_fun():
    # Borderline-SMOTE for imbalanced dataset.
    x,y = Definedata()

    # Summarize the class dist.
    counter = Counter(y)
    print(counter)

    # transform the data
    smt = SMOTE(random_state=42)
    x,y = smt.fit_resample(x,y)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)

    # Summarize the new class distribution
    counter = Counter(y)
    print(counter)

    # SCatter plot of example by class labels.
    for label, _ in counter.items():
        row_ix = where(y == label)[0]
        plt.scatter(x[row_ix,0],x[row_ix,1], label = str(label))
    plt.legend()
    plt.show()
    return x_train, x_test, y_train, y_test

# MAchine learning modeling:-

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

def Models(model, x_train, x_test, y_train, y_test, title):
    model.fit(x_train, y_train)

    x, y = make_classification(n_samples=1000, n_features=9, n_informative=2,
                               n_redundant=0, n_clusters_per_class=1,
                               weights=[0.9], random_state=42)

    train_matrix = pd.crosstab(y_train, model.predict(x_train), rownames=['Actual'], colnames=['Predicted'])
    test_matrix = pd.crosstab(y_test, model.predict(x_test), rownames=['Actual'], colnames=['Predicted'])
    matrix = pd.crosstab(y, model.predict(x), rownames=['Actual'], colnames=['Predicted'])

    f, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,7))
    g1 = sns.heatmap(train_matrix, annot=True, fmt='d', cbar=False, annot_kws={"size": 20}, ax=ax1)
    g1.set_title(title)
    g1.set_ylabel('Total career switch = {}'.format(y_train.sum()), fontsize=15)
    g1.set_xlabel('Accuracy = {:.2f}%'.format(accuracy_score(y_train, model.predict(x_train))), fontsize=15)

    g2 = sns.heatmap(test_matrix, annot=True, fmt='d', cbar=False, annot_kws={"size": 20}, ax=ax2)
    g2.set_ylabel('Total career switch = {}'.format(y_test.sum()), fontsize=15)
    g2.set_xlabel('Accuracy = {:.2f}%'.format(accuracy_score(y_test, model.predict(x_test))), fontsize=15)

    g3 = sns.heatmap(matrix, annot=True, fmt='d', cbar=False, annot_kws={"size": 20}, ax=ax3)
    g3.set_ylabel('Total career switch = {}'.format(y.sum()), fontsize=15)
    g3.set_xlabel('Accuracy = {:.2f}%'.format(accuracy_score(y, model.predict(x))), fontsize=15)

    plt.show()
    return y, model.predict(x)

def Featureimportance(model, x_train1, y_train1, df_test):
    model.fit(x_train1, y_train1)
    importances = model.feature_importances_
    features = df_test.columns[:9]
    imp = pd.DataFrame({'Features': features, 'Importance': importances})
    imp['sum importance'] = imp['Importance'].cumsum()
    imp = imp.sort_values(by='Importance')
    return imp

x_train1, x_test1, y_train1, y_test1 = SMOTE_fun()

# Building Logistic regression, KNN and Decision tree classifier.

title = "LogisticRegression / SMOTE"
Models(LogisticRegression(), x_train1, x_test1, y_train1, y_test1, title)

title = "KNN / SMOTE"
Models(KNeighborsClassifier(n_neighbors=1), x_train1, x_test1, y_train1, y_test1, title)

title = "Decision Tree / SMOTE"
Models(DecisionTreeClassifier(max_depth=14), x_train1, x_test1, y_train1, y_test1, title)

Featureimportance(DecisionTreeClassifier(max_depth=14), x_train1, y_train1, df_test)

